# clustering_min.py

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
import json
from sklearn.metrics.pairwise import cosine_similarity

from collections import defaultdict
from sentence_transformers import SentenceTransformer
import numpy as np
from sentence_transformers import models

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

from clustering_db_handler import update_cluster_id

# ä»®ã®ãƒ¡ãƒ¢ãƒ‡ãƒ¼ã‚¿
memos = [
    {"id": 1, "text": "é­šä»‹ç³»ã®ã‚¹ãƒ¼ãƒ—ãŒåŠ¹ã„ã¦ã„ãŸãƒ©ãƒ¼ãƒ¡ãƒ³"},
    {"id": 2, "text": "å¡©ãƒ©ãƒ¼ãƒ¡ãƒ³ã®ã‚¹ãƒ¼ãƒ—ãŒé€ãé€šã£ã¦ã„ãŸ"},
    {"id": 3, "text": "è¾›å‘³å™Œãƒ©ãƒ¼ãƒ¡ãƒ³ã«ãƒãƒ£ãƒ¼ã‚·ãƒ¥ãƒ¼ãŒåˆã†"},
    {"id": 4, "text": "å®¶ç³»ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’ä¹…ã€…ã«é£Ÿã¹ãŸ"},
    {"id": 5, "text": "å‘³å™Œãƒ©ãƒ¼ãƒ¡ãƒ³ãŒæœ€é«˜ã ã£ãŸ"},
    {"id": 6, "text": "ã¤ã‘éººã®ã‚³ã‚·ãŒè‰¯ã‹ã£ãŸ"},
    {"id": 7, "text": "äººé–“ã®å­˜åœ¨æ„ç¾©ã«ã¤ã„ã¦è€ƒãˆãŸ"},
    {"id": 8, "text": "ç”Ÿãã‚‹æ„å‘³ã¯ä¸»è¦³çš„ã‹ã‚‚ã—ã‚Œãªã„"},
    {"id": 9, "text": "è‡ªç”±æ„å¿—ã¨ã¯å¹»æƒ³ãªã®ã‹"},
    {"id": 10, "text": "ç¤¾ä¼šçš„å½¹å‰²ã«ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯ç¸›ã‚‰ã‚Œã‚‹ã®ã‹"},
    {"id": 11, "text": "å€«ç†ã¯æ–‡åŒ–ã«ã‚ˆã£ã¦ç›¸å¯¾çš„ã‹"},
    {"id": 12, "text": "æ­»å¾Œã®ä¸–ç•Œã¯ç§‘å­¦ã§èª¬æ˜ã§ãã‚‹ã®ã‹"},
    {"id": 13, "text": "Pythonã¯å­¦ç¿’ã‚³ã‚¹ãƒˆãŒä½ãäººæ°—"},
    {"id": 14, "text": "Reactã¯UIé–‹ç™ºã«å¼·ã„"},
    {"id": 15, "text": "Webã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¸¸ã«å¤‰åŒ–ã™ã‚‹"},
    {"id": 16, "text": "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ãƒ•ãƒ­ãƒ³ãƒˆã®åˆ†é›¢ãŒé‡è¦"},
    {"id": 17, "text": "Dockerã§ã®ç’°å¢ƒæ§‹ç¯‰ãŒç°¡å˜ã«ãªã£ãŸ"},
    {"id": 18, "text": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨­è¨ˆãŒé›£ã—ã„"},
    {"id": 19, "text": "ä»Šæ—¥ã¯é›¨ã ã£ãŸã®ã§å®¶ã§èª­æ›¸ã—ãŸ"},
    {"id": 20, "text": "æœ€è¿‘ã€ç¡çœ ã®è³ªãŒæ‚ªã„"},
    {"id": 21, "text": "æ–­æ¨é›¢ã‚’ã—ã¦éƒ¨å±‹ãŒã™ã£ãã‚Šã—ãŸ"},
    {"id": 22, "text": "æ•£æ­©ä¸­ã«çŒ«ã‚’è¦‹ã‹ã‘ãŸ"},
    {"id": 23, "text": "æœé£Ÿã«ãƒ‘ãƒ³ã¨ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’ã¨ã£ãŸ"},
    {"id": 24, "text": "å¤œæ›´ã‹ã—ãŒç¶šã„ã¦ä½“èª¿ãŒæ‚ªã„"},
    {"id": 25, "text": "å¤±æ•—ã‹ã‚‰å­¦ã¶å§¿å‹¢ãŒå¤§äº‹ã ã¨æ€ã£ãŸ"},
    {"id": 26, "text": "è‡ªå·±è‚¯å®šæ„ŸãŒä¸‹ãŒã£ã¦ã„ã‚‹"},
    {"id": 27, "text": "ã‚„ã‚‹æ°—ãŒå‡ºãªã„åŸå› ã‚’è€ƒãˆã¦ã„ãŸ"},
    {"id": 28, "text": "ç›®æ¨™ã‚’æŒãŸãªã„ã¨æ—¥ã€…æµã•ã‚Œã‚‹"},
    {"id": 29, "text": "å°ã•ãªæˆåŠŸä½“é¨“ã‚’ç©ã¿ä¸Šã’ãŸã„"},
    {"id": 30, "text": "æ„Ÿæƒ…ã‚’è¨€èªåŒ–ã™ã‚‹ã“ã¨ã¯é›£ã—ã„"},
    {"id": 31, "text": "ç«æ˜Ÿæ¢æŸ»æ©Ÿã®æ‰“ã¡ä¸Šã’ã«æˆåŠŸã—ãŸ"},
    {"id": 32, "text": "æ ªä¾¡ãŒæ€¥ä¸Šæ˜‡ã—çµŒæ¸ˆã«å½±éŸ¿ã‚’ä¸ãˆãŸ"},
    {"id": 33, "text": "æ—¥æœ¬ã®ä¼çµ±èŠ¸èƒ½ã§ã‚ã‚‹æ­Œèˆä¼ã‚’è¦³è³ã—ãŸ"},
    {"id": 34, "text": "ã‚ªãƒ¼ãƒ­ãƒ©ãŒè¦‹ãˆã‚‹åŒ—æ¬§ã¸ã®æ—…è¡Œã‚’è¨ˆç”»ã—ã¦ã„ã‚‹"},
    {"id": 34, "text": "èƒå…ã‚ˆé€€æ²»ã‚ˆãªãœè¸Šã‚‹ã€‚æ¯è¦ªã®å¿ƒãŒã‚ã‹ã£ã¦æã‚ã—ã„ã®ã‹ã€‚ã§ã¯ã˜ã¾ã‚‹å°èª¬ã¯å¤¢é‡ä¹…ä½œã®ãƒ‰ã‚°ãƒ©ãƒã‚°ãƒ©ã§ã™"}
]
# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
texts = [m["text"] for m in memos]

# ransformer ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
bert = models.Transformer("cl-tohoku/bert-base-japanese-v3")

# ãƒ—ãƒ¼ãƒªãƒ³ã‚°å±¤ã‚’è¿½åŠ 
pooling = models.Pooling(
    bert.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True
)

# 3. SentenceTransformer ã¨ã—ã¦çµ±åˆ
model = SentenceTransformer(modules=[bert, pooling])


embeddings = model.encode(texts)

clustering = AgglomerativeClustering(
    n_clusters=None, distance_threshold=0.2, metric='cosine',      # cosine é¡ä¼¼åº¦ã§è·é›¢è¨ˆç®—
    linkage='average'
)
labels = clustering.fit_predict(embeddings)

# group_id ã‚’ãƒ¡ãƒ¢ã«è¿½åŠ 
for i, label in enumerate(labels):
    memos[i]["cluster_id"] = int(label) + 1  # 1ã‹ã‚‰å§‹ã¾ã‚‹IDã«ã™ã‚‹

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
for m in memos:
    update_cluster_id(m["id"], m["cluster_id"])



# çµæœã‚’JSONã§å‡ºåŠ›
print(json.dumps(memos, ensure_ascii=False, indent=2))
sim = cosine_similarity(embeddings)
print("\nğŸ“Š é¡ä¼¼åº¦è¡Œåˆ—ï¼ˆcosine similarityï¼‰:")
for row in sim:
    print(["{0:.2f}".format(val) for val in row])

print("\nğŸ“Œ å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆTF-IDFï¼‰:")


# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ãƒ¡ãƒ¢ã‚’ã¾ã¨ã‚ã‚‹
clustered = defaultdict(list)
for m in memos:
    clustered[m["cluster_id"]].append(m["text"])

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«å‡ºåŠ›
for gid, texts in clustered.items():
    print(f"\nğŸ§  Group {gid} ({len(texts)}ä»¶)")
    for t in texts:
        print(f" - {t}")

# PCAã§æ¬¡å…ƒã‚’2ã«åœ§ç¸®
pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)

# group_idã”ã¨ã«è‰²åˆ†ã‘ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(10, 6))
for gid in set(labels):
    idx = [i for i, l in enumerate(labels) if l == gid]
    plt.scatter(reduced[idx, 0], reduced[idx, 1], label=f'Group {gid}', alpha=0.6)

# ãƒ©ãƒ™ãƒ«ã¤ã‘ã¦è¡¨ç¤º
plt.title("ğŸ§  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆï¼ˆPCAï¼‰")
plt.xlabel("ä¸»æˆåˆ†1")
plt.ylabel("ä¸»æˆåˆ†2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
