# clustering_min.py

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
import json
from sklearn.metrics.pairwise import cosine_similarity
import MeCab


# MeCab Taggerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
mecab = MeCab.Tagger()

# åˆ†ã‹ã¡æ›¸ãç”¨ã®é–¢æ•°å®šç¾©
def tokenize(text):
    tagger = MeCab.Tagger("-Owakati")
    return tagger.parse(text).strip()

# ä»®ã®ãƒ¡ãƒ¢ãƒ‡ãƒ¼ã‚¿
memos = [
     {"id": 1, "text": "é­šä»‹ç³»ã®ã‚¹ãƒ¼ãƒ—ãŒåŠ¹ã„ã¦ã„ãŸãƒ©ãƒ¼ãƒ¡ãƒ³"},
    {"id": 2, "text": "å¡©ãƒ©ãƒ¼ãƒ¡ãƒ³ã®ã‚¹ãƒ¼ãƒ—ãŒé€ãé€šã£ã¦ã„ãŸ"},
    {"id": 3, "text": "è¾›å‘³å™Œãƒ©ãƒ¼ãƒ¡ãƒ³ã«ãƒãƒ£ãƒ¼ã‚·ãƒ¥ãƒ¼ãŒåˆã†"},
    {"id": 4, "text": "å®¶ç³»ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’ä¹…ã€…ã«é£Ÿã¹ãŸ"},
    {"id": 5, "text": "å‘³å™Œãƒ©ãƒ¼ãƒ¡ãƒ³ãŒæœ€é«˜ã ã£ãŸ"},
    {"id": 6, "text": "ã¤ã‘éººã®ã‚³ã‚·ãŒè‰¯ã‹ã£ãŸ"},
    {"id": 7, "text": "äººé–“ã®å­˜åœ¨æ„ç¾©ã«ã¤ã„ã¦è€ƒãˆãŸ"},
    {"id": 8, "text": "ç”Ÿãã‚‹æ„å‘³ã¯ä¸»è¦³çš„ã‹ã‚‚ã—ã‚Œãªã„"},
    {"id": 9, "text": "è‡ªç”±æ„å¿—ã¨ã¯å¹»æƒ³ãªã®ã‹"},
    {"id": 10, "text": "ç¤¾ä¼šçš„å½¹å‰²ã«ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¯ç¸›ã‚‰ã‚Œã‚‹ã®ã‹"},
    {"id": 11, "text": "å€«ç†ã¯æ–‡åŒ–ã«ã‚ˆã£ã¦ç›¸å¯¾çš„ã‹"},
    {"id": 12, "text": "æ­»å¾Œã®ä¸–ç•Œã¯ç§‘å­¦ã§èª¬æ˜ã§ãã‚‹ã®ã‹"},
    {"id": 13, "text": "Pythonã¯å­¦ç¿’ã‚³ã‚¹ãƒˆãŒä½ãäººæ°—"},
    {"id": 14, "text": "Reactã¯UIé–‹ç™ºã«å¼·ã„"},
    {"id": 15, "text": "Webã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å¸¸ã«å¤‰åŒ–ã™ã‚‹"},
    {"id": 16, "text": "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ãƒ•ãƒ­ãƒ³ãƒˆã®åˆ†é›¢ãŒé‡è¦"},
    {"id": 17, "text": "Dockerã§ã®ç’°å¢ƒæ§‹ç¯‰ãŒç°¡å˜ã«ãªã£ãŸ"},
    {"id": 18, "text": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¨­è¨ˆãŒé›£ã—ã„"},
    {"id": 19, "text": "ä»Šæ—¥ã¯é›¨ã ã£ãŸã®ã§å®¶ã§èª­æ›¸ã—ãŸ"},
    {"id": 20, "text": "æœ€è¿‘ã€ç¡çœ ã®è³ªãŒæ‚ªã„"},
    {"id": 21, "text": "æ–­æ¨é›¢ã‚’ã—ã¦éƒ¨å±‹ãŒã™ã£ãã‚Šã—ãŸ"},
    {"id": 22, "text": "æ•£æ­©ä¸­ã«çŒ«ã‚’è¦‹ã‹ã‘ãŸ"},
    {"id": 23, "text": "æœé£Ÿã«ãƒ‘ãƒ³ã¨ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’ã¨ã£ãŸ"},
    {"id": 24, "text": "å¤œæ›´ã‹ã—ãŒç¶šã„ã¦ä½“èª¿ãŒæ‚ªã„"},
    {"id": 25, "text": "å¤±æ•—ã‹ã‚‰å­¦ã¶å§¿å‹¢ãŒå¤§äº‹ã ã¨æ€ã£ãŸ"},
    {"id": 26, "text": "è‡ªå·±è‚¯å®šæ„ŸãŒä¸‹ãŒã£ã¦ã„ã‚‹"},
    {"id": 27, "text": "ã‚„ã‚‹æ°—ãŒå‡ºãªã„åŸå› ã‚’è€ƒãˆã¦ã„ãŸ"},
    {"id": 28, "text": "ç›®æ¨™ã‚’æŒãŸãªã„ã¨æ—¥ã€…æµã•ã‚Œã‚‹"},
    {"id": 29, "text": "å°ã•ãªæˆåŠŸä½“é¨“ã‚’ç©ã¿ä¸Šã’ãŸã„"},
    {"id": 30, "text": "æ„Ÿæƒ…ã‚’è¨€èªåŒ–ã™ã‚‹ã“ã¨ã¯é›£ã—ã„"}
]
# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
texts = [tokenize(m["text"]) for m in memos]
vectorizer = TfidfVectorizer(ngram_range=(1, 2))
X = vectorizer.fit_transform(texts)

# éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆè·é›¢ã—ãã„å€¤ã§è‡ªå‹•ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
clustering = AgglomerativeClustering(
    n_clusters=None,              # è‡ªå‹•ã§ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ±ºå®š
    distance_threshold=0.85,       # ã—ãã„å€¤ï¼ˆå°ã•ã„ã»ã©åˆ†ã‹ã‚Œã‚‹ï¼‰
    metric='cosine',
    linkage='average'
)
labels = clustering.fit_predict(X.toarray())

# group_id ã‚’ãƒ¡ãƒ¢ã«è¿½åŠ 
for i, label in enumerate(labels):
    memos[i]["group_id"] = int(label)

# çµæœã‚’JSONã§å‡ºåŠ›
print(json.dumps(memos, ensure_ascii=False, indent=2))
sim = cosine_similarity(X.toarray())
print("\nğŸ“Š é¡ä¼¼åº¦è¡Œåˆ—ï¼ˆcosine similarityï¼‰:")
for row in sim:
    print(["{0:.2f}".format(val) for val in row])

